## Homework 2

---
title: "Machine Learning for Health Care: Homework 2"
output:
  html_document:
  fig_width: 7
fig_height: 5
---
### Author: Xin Gao, xing1
## Overview
Homework 2 is about applying what you have learned in class into analysis in R. You will draw from both your learning in lecture and discussion with the skills you are developing in the workshop sessions.

The homework is split into two parts: short questions to illustrate concepts, and a secondary analysis of data from a randomized controlled trial.

**Homework 2 is due March 6th at the beginning of class.**

### Data set
The data set used for this homework comes from the International Stroke Trial. This was a study comparing the effectiveness of medications in a populaton of patients who had suffered strokes. The publication was in the leading British medical journal Lancet:
http://www.sciencedirect.com/science/article/pii/S0140673697040117 (you may need to be on campus or use VPN)

The data set is here:
http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_corrected.csv
(more information here: http://datashare.is.ed.ac.uk/handle/10283/128)

The variable definitions files are also helpful:
http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_variables.pdf
http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_variables.csv

## Objectives
- git
- debug
- inject belief/knowledge by shifting from ML to MAP estimates
- choosing MCAR, MAR, MNAR; choosing indicator and/or imputation
- run machine learning algorithms: LR, NB, TAN, decision tree
- reporting performance, using ggplot

## Instructions

For this homework, you will use git. **To submit the homework, email me a link to your git repository.** I should be able to type "git clone <url>" and have it download from a cloud service (github, bitbucket, etc). Note that if it is a private repository, you will need to permit me access to it (please provide access to jeremy.weiss@gmail.com).

Your git repository should contain at least two commits with useful comments on what has changed from the previous version(s). This should be visible when I type in ```git log```. The submission I will grade is at the HEAD revision unless specified otherwise in your email. Include your .Rmd file and your .html file solutions in the repository with your name and andrew ID.

  
## Part 1: Concept questions (6 points)

The code that follows introduces a toy data set, decision tree model, and two prediction functions.
```{r eval=T, message=F}
library(dplyr)

# synthetic depression data
depressionData = data.frame( # do not change "depressionData"
  pregnant = c(1,0,1,1),
  depressed = c("yes","yes","no","no") %>% as.factor(),
  hospitalized = c(1, 0, 0, 0) %>% as.logical()
) %>% tbl_df()

# tree: a model that outputs the odds of hospitalization from inputs of data (datums)
tree = data.frame( # do not change "tree"
  splitVariable = c("depressed", "pregnant", NA, NA, NA),
  split = c("yes", 1, NA, NA, NA),
  trueChild = c(2, 4, NA, NA, NA),
  falseChild = c(3, 5, NA, NA, NA),
  odds = c(NA, NA, 0.1, 2, 3)
)

predictOddsOnDataSet = function(tree, data, active = 1) {
  apply(data, 1, (function(x) {predictedOdds(tree=tree, x, active=1)})  )
}

predictedOdds = function(tree, datum, active = 1) {
  
  if(is.na(tree[active,"splitVariable"])) { # leaf of tree, so output value
    
    return(tree$odds[active])
    
  } else {                                  # internal node of tree, so continue down tree to true/false child
    
    if( (datum[[tree[active,"splitVariable"] %>% as.character]] %>% as.character) == tree[active,"split"])
      return(predictedOdds(tree, datum, active = tree[active,"trueChild"]))
    
    else
      return(predictedOdds(tree, datum, active = tree[active,"falseChild"]))
    
  }
  
}

predictOdds<- predictOddsOnDataSet(tree,depressionData)
newDepressionData <- depressionData %>% mutate (predictPro = predictOdds/(1+predictOdds))
newDepressionData

TP<-sum(newDepressionData$hospitalized==TRUE & newDepressionData$predictPro>=0.5)
FP<-sum(newDepressionData$hospitalized==FALSE & newDepressionData$predictPro>=0.5)
FN<-sum(newDepressionData$hospitalized==TRUE & newDepressionData$predictPro<0.5)
TN<-sum(newDepressionData$hospitalized== FALSE & newDepressionData$predictPro<0.5)
accuracy<- (TP+TN)/(TP+FP+FN+TN)
accuracy
precision <-TP/(TP+FP)
precision
recall<-TP/(TP+FN)
recall
sensitivity <-recall
sensitivity
specificity <-1-(FP/(TN+FP))
specificity

nDiabetes = 5
nHealthy = 10-5

priorNumOfDiabetes = 0
priorNumOfHealthy = 0

xat = seq(0,1,0.001)
MLE <-data.frame(y = dbeta(xat,shape1 = priorNumOfDiabetes+nDiabetes+1,shape2 = priorNumOfHealthy+nHealthy+1),
                 x = xat)
MLE<-MLE$x[which.max(MLE$y)]
MLE

# MLE = nDiabetes/(nDiabetes+nHealthy)

nDiabetes = 5
nHealthy = 10-5

priorNumOfDiabetes = 11
priorNumOfHealthy = 21

xat = seq(0,1,0.001)
MAP <-data.frame(y = dbeta(xat,shape1 = priorNumOfDiabetes+nDiabetes+1,shape2 = priorNumOfHealthy+nHealthy+1),
                 x = xat)
MAP<-MAP$x[which.max(MAP$y)]
MAP

# goal: run predictOddsOnDataSet(tree, depressionData)
```
  
First, verify to yourself that, for the fourth patient in ```depressionData```, the tree should have output an odds of 0.1.

Fix the function ```predictedOdds``` so that ```predictedOddsOnDataSet``` outputs the odds for each patient in data. Use the debugger functions like ```debugonce(predictedOdds)``` or ```browser()``` to inspect the code. 

What did you change?

Syntax error. quote "trueChild" "falseChild" in the function ```predictedOdds```.
Actually,it's correct in your html version

Add a column of the predicted probabilities of hospitalization to depressionData. Display it.
[response required]

See above

Using a threshold probability of 0.5, what is:

- the accuracy of the model? 
0.75
- the sensitivity of the model?
1
- the specificity of the model?
0.667
- the precision of the model?
0.5
- the recall of the model?
1


Suppose you want to know the prevalence of diabetes in Pittsburgh. If you randomly survey 10 Pittsburghers and 5 of them state they have diabetes:

- what is the maximum likelihood estimate for the prevalence of diabetes?

p(diabetes) = 0.5

- given your strong belief specified by a beta prior of $\alpha = 11, \beta = 21$, what is the maximum a posteriori estimate for the prevalence of diabetes?

p(diabetes) = 0.381

## Part 2: Analysis (9 points)

#### Preliminaries
- **Y:** What was the definition of the primary outcome in this study?
- What is (are) the variable name(s) for the outcome?
Primary outcome: death within 14 days and death or dependency at 6 months

Variable: 

ID14 (indicator of death at 14 days)

OCCODE(6 month outcome(1-dead/2-dependent/3-not recovered/4-recovered/8 or9 missing))

- **U:** what is (are) the variable name(s) for the intervention, and what is (are) their possible values?

Intervention: immediate heparin (low or medium dose),avoid heparin, immediate aspirin, avoid aspirin

Variable:

RXASP(Trial aspirin allocated(Y/N))

RXHEP(Trial heparin allocated(M/L/N))

- **V, W:** describe the covariates included and the population being studied.

Covariates:

Delay (h from symptoms); Age; Sex; Onset; Conscious level; Cardiac rhythm; Systolic BP; Stroke syndrome; Leg weakness; CT scan; Apperance of pre-randomisation CT; Pre-randomisation antithrombotic therapy

Population:

19435 patients with suspected acute ischaemic stroke within 48 hours of symptom onset, with no evidence of intracranial haemorrhage or clear indication for or contraindications to heparin or asprin  

- Construct a so-called Table 1 for groups of {aspirin, no aspirin} use, including information on age, gender, systolic blood pressure, and conscious state.


```{r eval=T, message=F}
IST <-read.csv(file = 'http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_corrected.csv',header = TRUE,
na.strings = c("","NA"))
table<- IST %>% select(3,4,5,12,26)
Age<-cut(as.numeric(IST$AGE),c(0,50,60,70,80,Inf))
RSBP <-cut(as.numeric(IST$RSBP),c(0,140,160,180,Inf))

table(IST$SEX,IST$RXASP)
table(Age,IST$RXASP)
table(RSBP,IST$RXASP)
table(IST$RCONSC,IST$RXASP)
```

#### Machine learning analysis
Note: for this analysis, use a simple 50-50 train-test split.

Let our outcome of interest be "dead or dependent at 6 months", i.e. so that we have a binary classification problem. What percent of patients are dead or dependent at 6 months in your train set and test set?
```{r eval=T, message=F}
library(caret)
library(plyr)
IST$OCCODE <-mapvalues(IST$OCCODE, from = c(1,2,3,4,8,9), to = c(1,1,0,0,0,0))
trainIndex = sample(1:nrow(IST),size = round(0.5*nrow(IST)),replace = FALSE)
train = IST[trainIndex,]
test = IST[-trainIndex,]

percentTrain = sum(train$OCCODE==1)/sum(train$OCCODE==1, train$OCCODE==0)
percentTrain

percentTest = sum(test$OCCODE==1)/sum(test$OCCODE==1, test$OCCODE==0)
percentTest
```
62.5% of patients are dead or depedent at 6 months in my training set
62.3% of patients are dead or depedent at 6 months in my test set

Choose which variables to include in your model. For example, remove variables for outcomes at 14 days (because if you are dead at 14 days you are certainly dead at 6 months). Moreover, you should remove all features measured after baseline if you want to make a prediction based on baseline data. Similarly, specific indicators of the outcome should also be removed, since those are measurements past the baseline that are not our outcome of interest. For these reasons, you will need to remove clusters of variables. Justify your approach.
[response required]
```{r eval=T, message=F}
y<-c("OCCODE")
u<-c("RXASP","RXHEP")
v<-c("RDELAY","RCONSC","SEX","AGE","RSLEEP","RATRIAL", "RVISINF","RHEP24","RASP3","RSBP","STYPE","FAP","FOAC")
x<-c(y,u,v)
train<-train[x]
test<-test[x]
```
Of the remaining variables, decide whether to exclude variables with missing data, impute them, and/or use indicator variables. (Note that if you choose multiple imputation for some variables, you would need to pool the results when evaluating performance, however for homework you may just use the first imputed data set). Justify your approach.
[response required]
```{r eval=T, message=F}
# trainig set
library(lattice)
library(mice)
for (i in x){
  train<- train %>% tbl_df() %>% filter(!is.na(i))
}
train$OCCODE = train$OCCODE==1
missing = md.pattern(train);missing

# remove RATRIAL,RASP3,FAP,FOAC
# those variables have so many missing values that should not be considered as missinging at random
train.sub <-subset(train,select=-c(RATRIAL,RASP3,FAP,FOAC))
# imputate
mTrain = mice(data = train.sub, m = 5, maxit=2, seed = 0)
train <-complete(mTrain)

# test set
library(lattice)
library(mice)
for (i in x){
  test<- test %>% tbl_df() %>% filter(!is.na(i))
}
test$OCCODE = test$OCCODE==1
missing = md.pattern(test);missing

# remove RATRIAL,RASP3,FAP,FOAC
test.sub <-subset(test,select=-c(RATRIAL,RASP3,FAP,FOAC))
# imputate
mTest = mice(data = test.sub, m = 5, maxit=2, seed = 0)
test <-complete(mTest)
```

Use the following machine learning algorithms: logistic regression, naive Bayes, Tree Augmented Naive Bayes, and decision tree (specify any parameters you set that are not the default). The packages that you may find useful here are: "glm", "bnlearn", and "rpart", but you may use others if desired. In a table, report the accuracy with 95% confidence intervals for each algorithm.
[response required]

```{r eval=T, warning=F, message=F}
# Logistic Regression
library(dummies)
train2<-dummy.data.frame(train)
test2<-dummy.data.frame(test)
lrtrain <- glm(OCCODE==TRUE~., family = binomial("logit"),data=train2 )
lrpredict <- predict(lrtrain,test2,type = "response")

lrtbl<-table(lrpredict>=0.5,test2$OCCODE)
lrtotal<-sum(lrtbl)
lracc<- (lrtbl[1,1]+lrtbl[2,2])/lrtotal
lrerror<-1-lracc
lrCI.lower<-lracc-1.96*sqrt(lrerror*lracc/lrtotal)
lrCI.upper<-lracc+1.96*sqrt(lrerror*lracc/lrtotal)
data.frame(lrCI.lower,lracc,lrCI.upper)

# Naive Bayes
# combine training set and test set
library(bnlearn)
nbData<-bind_rows(train,test)# combine datasets for discretization
nbTotal <-nrow(nbData)
# convert features that are integers but really categorical
nbData[,sapply(nbData, (function(x) length(unique(x))<6))] = lapply(nbData[,sapply(nbData, (function(x) length(unique(x))<6))], as.factor) 
# convert integer features to doubles
nbData[,sapply(nbData, is.integer)] = lapply(nbData[,sapply(nbData, is.integer)], as.numeric) 
datadiscrete = discretize(nbData)
ordering = sample(1:nrow(datadiscrete))
nbtrain = datadiscrete[ordering[1:nbTotal/2],]
nbtest = datadiscrete[-ordering[1:nbTotal/2],]

nb = naive.bayes(nbtrain, "OCCODE")
fitted = bn.fit(nb, nbtrain)
nbtbl<-predict(fitted, nbtest) %>% table(nbtest$OCCODE)
nbtotal<-nrow(nbtest)
nbacc<- (nbtbl[1,1]+nbtbl[2,2])/nbtotal
nberror<-1-nbacc
nbCI.lower<-nbacc-1.96*sqrt(nberror*nbacc/nbtotal)
nbCI.upper<-nbacc+1.96*sqrt(nberror*nbacc/nbtotal)
data.frame(nbCI.lower,nbacc,nbCI.upper)

# Tree Augmented NB
tan = tree.bayes(nbtrain, "OCCODE")
fittedTan = bn.fit(tan, nbtrain)
tantbl<- predict(fittedTan, nbtest) %>% table(nbtest$OCCODE)
tantotal<-nrow(nbtest)
tanacc<- (tantbl[1,1]+tantbl[2,2])/tantotal
tanerror<-1-tanacc
tanCI.lower<-tanacc-1.96*sqrt(tanerror*tanacc/tantotal)
tanCI.upper<-tanacc+1.96*sqrt(tanerror*tanacc/tantotal)
data.frame(tanCI.lower,tanacc,tanCI.upper)

# Decision Tree
library(rpart)
dttrain <-rpart(OCCODE~., data=train,method="class",control=rpart.control(minsplit = 30,cp=0.001))
printcp(dttrain)
plot(dttrain)
pfit <-prune(dttrain,cp= dttrain$cptable[which.min(dttrain$cptable[,"xerror"]),"CP"])
dtpredict <-predict(pfit,test,type ="class")
dttbl<-table(dtpredict,test$OCCODE)
dttotal<-nrow(test)
dtacc<- (dttbl[1,1]+dttbl[2,2])/dttotal
dterror<-1-dtacc
dtCI.lower<-dtacc-1.96*sqrt(dterror*dtacc/dttotal)
dtCI.upper<-dtacc+1.96*sqrt(dterror*dtacc/dttotal)
data.frame(dtCI.lower,dtacc,dtCI.upper)

```
Construct an ROC (receiver operating characteristic) curve for each model and overlay them on a graph using ggplot. Include a legend. Hint: you will find the package "ROCR" helpful (or you might try the package "precrec", but I have not tested it).
[response required]
```{r eval=T, warning=F, message=F}
library(ROCR)
# Logistic regression
lrProbs = predict(lrtrain,test2,type = "response")
lrcomparison <- data.frame(predictions = lrProbs,actual = test2["OCCODE"]) %>% tbl_df()
lrROC = prediction(lrcomparison[[1]],lrcomparison[[2]])%>% performance("tpr","fpr")

ggdisplay = function(perf, title="ROC for Logistic Regression ", xlab = "fpr", ylab = "tpr") {
  ggperf = data.frame(y = perf@y.values[[1]],
                      x = perf@x.values[[1]])
  ggplot(data = ggperf, aes(x=x,y=y)) +
    geom_line() +
    coord_cartesian(xlim=c(0,1),ylim=c(0,1))+
    xlab(xlab) + ylab(ylab) +
    ggtitle(title)
}
ggdisplay(lrROC)

# Naive Bayes
nbProbs = predict(fitted,nbtest,prob=T)%>% attr("prob")
nbcomparison = data.frame(predictions = nbProbs["TRUE",], actual= nbtest["OCCODE"]) %>% tbl_df()
nbROC = prediction(nbcomparison[[1]], nbcomparison[[2]]) %>% performance("tpr","fpr")
ggdisplay = function(perf, title="ROC for Naive Bayes ", xlab = "fpr", ylab = "tpr") {
  ggperf = data.frame(y = perf@y.values[[1]],
                      x = perf@x.values[[1]])
  ggplot(data = ggperf, aes(x=x,y=y)) +
    geom_line() +
    coord_cartesian(xlim=c(0,1),ylim=c(0,1))+
    xlab(xlab) + ylab(ylab) +
    ggtitle(title)
}
ggdisplay(nbROC)

# Tree Agumented Naive Bayes
tanProbs = predict(fittedTan, nbtest, prob=T) %>% attr("prob")
tancomparison = data.frame(predictions = tanProbs["TRUE",], actual= nbtest["OCCODE"]) %>% tbl_df()
tanROC = prediction(tancomparison[[1]], tancomparison[[2]]) %>% performance("tpr","fpr")

ggdisplay = function(perf, title="ROC for Tree Augmented Naive Bayes ", xlab = "fpr", ylab = "tpr") {
  ggperf = data.frame(y = perf@y.values[[1]],
                      x = perf@x.values[[1]])
  ggplot(data = ggperf, aes(x=x,y=y)) +
    geom_line() +
    coord_cartesian(xlim=c(0,1),ylim=c(0,1))+
    xlab(xlab) + ylab(ylab) +
    ggtitle(title)
}
ggdisplay(tanROC)

# Decision Tree
dtProbs = predict(pfit,test,type="prob")
dtcomparison = data.frame(predictions = dtProbs[,2], actual= test["OCCODE"]) %>% tbl_df()
dtROC = prediction(dtcomparison[[1]], dtcomparison[[2]]) %>% performance("tpr","fpr")
ggdisplay = function(perf, title="ROC for Decision Tree ", xlab = "fpr", ylab = "tpr") {
  ggperf = data.frame(y = perf@y.values[[1]],
                      x = perf@x.values[[1]])
  ggplot(data = ggperf, aes(x=x,y=y)) +
    geom_line() +
    coord_cartesian(xlim=c(0,1),ylim=c(0,1))+
    xlab(xlab) + ylab(ylab) +
    ggtitle(title)
}
ggdisplay(dtROC)
```

Construct a PR (precision recall) curve for each model. Include a legend.
[response required] 
```{r eval=T, warning=F, message=F}

# Logistic Regression
lrPR = prediction(lrcomparison[[1]],lrcomparison[[2]]) %>% performance("prec","rec")
ggdisplay(lrPR, "PR Curve for Logistic Regression", "recall", "precision")
# Naive Bayes
nbPR = prediction(nbcomparison[[1]],nbcomparison[[2]]) %>% performance("prec","rec")
ggdisplay(nbPR, "PR Curve for Naive Bayes", "recall", "precision")
# Tree Augmented Naive Bayes
tanPR = prediction(tancomparison[[1]],tancomparison[[2]]) %>% performance("prec","rec")
ggdisplay(tanPR, "PR Curve for Tree Augmented Naive Bayes", "recall", "precision")
# Decision Tree
dtPR = prediction(dtcomparison[[1]],dtcomparison[[2]]) %>% performance("prec","rec")
ggdisplay(dtPR, "PR Curve for Decision Tree", "recall", "precision")
```


#### Conclusions
Let's draw conclusions from this study. Specifically,

- how well are we able to predict death or dependence at 6 months? [response required]

All four methods have an average accuracy of 0.7. 
Logistic regression is slightly better than other models with an accuracy of 0.709

- what is the average treatment effect of aspirin on death or dependence at 6 months? Is aspirin significantly better than the alternative? [response required]

ATE = P(OCCODE==1|RXASP==Y)-P(OCCODE==1|RXASP==N)=.622-.635 = -.013

- of the algorithms tested, which algorithms perform the best? Justify your statement.
[response required]

Logistic regression. 

Reason:1) higher accuracy (primary reason)
2) ROC curve is closer to left-top corner (but not significant) 3)PR curve above others(but not significant)

Congratulations, you've conducted a comparison of machine learning algorithms for mortality prediction! Commit your solutions to your git repository with an informative comment. ```git push``` will help you upload it to the cloud service you choose to use (github, bitbucket, etc).